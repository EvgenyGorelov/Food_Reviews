{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Use pretrained BERT for sentiment analysis\n",
    "\n",
    "For the predictive model I use pretrained bidirectional transformer [BERT](https://huggingface.co/transformers/model_doc/bert.html).  It is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. This method is found to be one of the best for text classification and it is especially suitable for the project due to relatively low training effort needed for fine tuning of the already pretrained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=2)\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                        num_labels=5)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random number seed for shuffling the training set should be used for maintaining shuffling in the case of Python runtime not keeping it's state between epochs (e.g. [Google Colab](https://colab.research.google.com)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_batching = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "=================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating of Training and Validation sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading clean and deduplicated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = pd.read_csv(\"./ReviewsDedupLowNT.csv.zip\",\n",
    "                       compression=\"zip\", \n",
    "                       keep_default_na=False,\n",
    "                       index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform review score to 0-based: from 1-5 stars to 0-4 label and rename column for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394998</th>\n",
       "      <td>these are sooooooooooooooooooooooooo delicious...</td>\n",
       "      <td>yummy. yummy, yummy!</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394999</th>\n",
       "      <td>this is a  for the price of  review because  p...</td>\n",
       "      <td>pure chocolate mallomars cookies</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395000</th>\n",
       "      <td>this would be a great coffee drink to grab out...</td>\n",
       "      <td>soso espresso style coffee drink</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395001</th>\n",
       "      <td>earths best infant formula soy iron,ounce is a...</td>\n",
       "      <td>earths best soy baby formula</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395002</th>\n",
       "      <td>taste. i was expecting it to taste pretty clos...</td>\n",
       "      <td>ehh okay i guess.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  \\\n",
       "394998  these are sooooooooooooooooooooooooo delicious...   \n",
       "394999  this is a  for the price of  review because  p...   \n",
       "395000  this would be a great coffee drink to grab out...   \n",
       "395001  earths best infant formula soy iron,ounce is a...   \n",
       "395002  taste. i was expecting it to taste pretty clos...   \n",
       "\n",
       "                                 Summary  Label  HelpfulnessNumerator  \\\n",
       "394998              yummy. yummy, yummy!      4                     0   \n",
       "394999  pure chocolate mallomars cookies      3                     0   \n",
       "395000  soso espresso style coffee drink      2                     1   \n",
       "395001      earths best soy baby formula      4                     0   \n",
       "395002                 ehh okay i guess.      1                     1   \n",
       "\n",
       "        HelpfulnessDenominator  \n",
       "394998                       3  \n",
       "394999                       0  \n",
       "395000                       2  \n",
       "395001                       0  \n",
       "395002                       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dedup[\"Score\"] = df_dedup[\"Score\"] - 1\n",
    "df_dedup.rename(columns={\"Score\": \"Label\"}, inplace=True)\n",
    "df_dedup.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is splitted into *train* (80% samples) and *test* (20%) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_dedup.sample(frac=0.8, random_state=123)\n",
    "df_test = df_dedup.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step a model predicting review score based on the review text will be created.\n",
    "For training of the BERT model the dataset samples should be converted to the *InputExample* objects, containing\n",
    "\n",
    "- *guid* - unique id for the example (not used)\n",
    "- *text_a* (string) - the untokenized text of the first sequence\n",
    "- *text_b* (optional, string) - the untokenized text of the second sequence (not used)\n",
    "- label (optional, int) - The label of the example\n",
    "\n",
    "and assembled to the TensorFlow datasets *train_data* and *validation_data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(df_input, data_column, label_column):\n",
    "    input_examples = df_input.apply(lambda x:\n",
    "                                    InputExample(guid=None,\n",
    "                                                 text_a=x[data_column],\n",
    "                                                 text_b=None,\n",
    "                                                 label=x[label_column]),\n",
    "                                    axis=1)\n",
    "    return input_examples\n",
    "\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,  # Adds [CLS] and [SEP].\n",
    "            max_length=max_length,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True)\n",
    "        input_ids, token_type_ids, attention_mask = (\n",
    "            input_dict[\"input_ids\"], input_dict[\"token_type_ids\"],\n",
    "            input_dict['attention_mask'])\n",
    "        features.append(InputFeatures(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield ({\n",
    "                \"input_ids\": f.input_ids,\n",
    "                \"attention_mask\": f.attention_mask,\n",
    "                \"token_type_ids\": f.token_type_ids, },\n",
    "                f.label, )\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen, ({\n",
    "            \"input_ids\": tf.int32,\n",
    "            \"attention_mask\": tf.int32,\n",
    "            \"token_type_ids\": tf.int32},\n",
    "            tf.int64),\n",
    "        ({\n",
    "            \"input_ids\": tf.TensorShape([None]),\n",
    "            \"attention_mask\": tf.TensorShape([None]),\n",
    "            \"token_type_ids\": tf.TensorShape([None]), },\n",
    "            tf.TensorShape([]), ), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_examples = convert_data_to_examples(\n",
    "    df_train, \"Text\", \"Label\") \n",
    "validation_input_examples = convert_data_to_examples(\n",
    "    df_test, \"Text\", \"Label\")\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(\n",
    "    list(train_input_examples), tokenizer)\n",
    "train_data = train_data.shuffle(\n",
    "    buffer_size=1000, seed=seed_batching).batch(32)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(\n",
    "    list(validation_input_examples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Treating sample imbalance\n",
    "\n",
    "The classes in our dataset are highly imbalanced. Rating 5 stars (class 4) is over 60 percent of all examples with the other classes hovering around 10 percent. In order to mitigate the impact of an unbalanced training set the [*SparseCategoricalFocalLoss*](https://focal-loss.readthedocs.io/en/latest/generated/focal_loss.SparseCategoricalFocalLoss.html#focal_loss.SparseCategoricalFocalLoss) loss function is used. This function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the focusing parameter that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAD/CAYAAABLjCkFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFQxJREFUeJzt3XmQXGW5x/FvCEEjAUFEAwQVCnggipRB1hsEDZE1gqDoLdYrSImCRcEtFXG5LOJCcSmgQAUplOUWt5QSDEu4GmWJoKBUiYA8KHtkEQoVCZuQuX+8p0nb9CSTYXr6ncz3U0WdmnOePv12k+lfn3Pe88yEgYEBJEmq0Ur9HoAkSYMxpCRJ1TKkJEnVMqQkSdUypCRJ1Vq53wNYkUTE64CtgEeBl/s8HEkaCyYC6wC3ZuYLnRsNqZG1FXBjvwchSWPQDsCCzpWG1Mh6FOCSSy5h6tSp/R6LJFXvscceY//994fm87OTITWyXgaYOnUq06ZN6/dYJGks6XqJxIkTkqRqDflIKiImAkcABwObUS523QdcCpyamc931L8X+CrlOs0U4E7gjMz8n0H2vwlwAjATWAv4E3AecHZmLu5Sv26z/9mUi24PAxcB3+p28S0i1gCOAz4MrA88DlwGnJCZT3epnwwcDRwAbAD8DbgK+Epmdj0slSSNrCEdSTUBdQVwFrAp8CvgOmBd4ETguoh4Q1v9bOAmYDdKOP0C2By4JCK+1mX/WwC3Ah8HHgTmUYLkTODCLvXTgF8Dh7MkPFZrxjIvIiZ11K8OXA98DlgMXNksjwFujog3dtRPal7vKc1+rwaeAg4DbouIty3zTZMkvWZDPd13GLAHcDuwaWbunJm7ARsDNwPbAF+GV45ALm4eNzszZ2XmHEpILQS+GBFbtnYcERMoQbQ6cGBmzszMfYBNmufbPyL27RjPOcA04MuZOSMzPwJsBPwM2An4bEf9ycC7KUdm0zPzo83+LwKmN9vbHUU5QrsK2LjZ/+bA14GpwNlDfN8kSa/BUEPqkGZ5dGb+ubUyM5+knAKEchQEcCDwFuCSzPxFW+29wOebH9tDZDYlQK7LzIvb6p9o2/cr9RERwJ7AvZQjnVb9IuBQysW3o9rq16CE7NPAsa1Th5n5EvBp4K/AoRGxalO/EuUIawA4snXqMDMHgC8BCewZERss7Q2TJL12Qw2pJ4G7gVu6bLunWa7bLHdtlpd3qb2SEiK7ta0btD4zbwL+AsyMiNWa1bsAE4C5ndeqMvMh4Dbg7RExvVn9PmAy8PPM/EdH/TOUo6/JwI7N6ncB6wG/y8wHOuoXAz9pfmx/DZKkHhhSSGXmnMzcrDla6bRVs1zYLN/ZLO/osp+ngUeAtSPircuqbz2sGWcrdJZVf3ez3HyU6iVJPfKapqA315NOan68rFmu0ywHmwHXWt8KqbFeL0nVePGf/enI1qvnfa03855COZ32OHBqs27VZvncII9prZ/SUf/sGK2XpGqsMmkic469YtSfd+5pe/Vkv8M+koqIE4EvAC8A+zUTHaBccxpoJhoM5blb8TtW6yVJPbLcH7QRsXJEfJcy5fx54MOZeUNbySJgQkS8fpBdTG6Wz7TVt68fa/WSpB5ZrpCKiCnAXJbcRLtLZl7TUfZIsxysw2rnNZ+xXi9J6pEhh1RErEnpMrErpQXRDh1HUC2tWXHTOzc0nR/WBZ7IzMeHUD+B0uHiZeCuZdU3NmuWvx+leklSjwy1LdIqlNZAW1LCYvvMHGyK9rxmuXeXbXMoPf+uHmL99sDawIK2e5xa9XOaG2/bx/k24D3Ag5nZCrUbKJMddm7dsNtWPwXYmXLq7kaAzPwDpTXTjIhYv6N+JeBDlOtV13YZryRpBA31SOpEYFvKEdROmblwKbWXUW7APSQidm+tjIgNgW9QPuD/u63+ekp/v9kR8cm2+rUp7Y8ATmutz8z7KUG1aTOuVv2qwPcoIdhevwj4AbAmcE5ErNzUr0xpb7QGcG7Hjb7fbvZzfkewnURpp/TjpoOGJKmHljkFPSLWYklboieA00tnolfLzAMy8+kmbC4DroyI64F/ALOANwDHZ+btbY9ZHBGfAOYD50bEoZTrQjtRguW8zJzb8VSfAX4JHB8Re1Fu+N2ecr3oGkrItDseeD9wEKV7xW3ADGBDSoeKr3bUn05pvTQb+GNE3AQEpRvFg8CRg75hkqQRM5QjqR1ZMqNtBrD/Uv4DIDN/0jzu/yin33akNIvdLzNf6bfXVn8LpUntZZSmtR+khMGnWNK/r73+PmBr4PuU04F7UHrwHQfs0/Tla69/ihJiZwKTKKcdFwPfAj7QtEdqr3+R0n7pJMr9UntS7os6F9jOP9UhSaNjwsDAsm4H0lBFxDuA++fPn+9f5pXUN2PpZt6FCxcya9YsgA06+6WCN6RKkipmSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqmVISZKqZUhJkqplSEmSqrXycB8YEYcAFwA7ZOaCLts3AU4AZgJrAX8CzgPOzszFXerXBb4KzAbWAR4GLgK+lZkvdKlfAzgO+DCwPvA4cBlwQmY+3aV+MnA0cACwAfA34CrgK5n5aJf6icBhwKeAjYHngPlN/T1LeWskSSNkWEdSEbEdcNZStm8B3Ap8HHgQmEcJkjOBC7vUTwN+DRzOkvBYDTgRmBcRkzrqVweuBz4HLAaubJbHADdHxBs76icBVwCnNPu9GniKEkK3RcTburyM7wHfAaYB1wL3Ax8DfhsR7xnstUuSRs5yh1RE7Ev50J4yyPYJlCBaHTgwM2dm5j7AJsDtwP7NPtqdQwmDL2fmjMz8CLAR8DNgJ+CzHfUnA++mHJlNz8yPNvu/CJjebG93FOUI7Spg42b/mwNfB6YCZ3e8hn2AQ4DbgI0yc9/M3JpyVDUF+H7zOiVJPTTkkIqIaRFxIfAjYCLl9Fo3sykBcl1mXtxamZlPAEc0P74SOhERwJ7AvZQjnVb9IuBQ4GVKyLTq16AcAT0NHNs6dZiZLwGfBv4KHBoRqzb1K1GOsAaAI1unDjNzAPgSkMCeEbFB22v4z2Z5TGb+vW1M3wV+2ry+nQZ/tyRJI2F5jqROBg4EfgNsC9w9SN2uzfLyzg2ZeRPwF2BmRKzWrN4FmADM7bxWlZkPUY5m3h4R05vV7wMmAz/PzH901D9DOfqaDOzYrH4XsB7wu8x8oKN+MfCT5sfd4JUQ3JZyOvDGLq/v8vZ6SVLvLE9I3Q0cDGyTmb9fSt07m+Udg2zP5nlbobOs+lYYbj5K9ZtRQvOubhM8utRLknpkyLP7MvMbQyxdp1m+asZcx/q3riD1kqQe6cV9Uqs2y2cH2f5cs2xNvBjr9ZKkHulFSL3cLAeG+NxjvV6S1CO9+KBd1CwnD7K9tf6ZFaRektQjvQipR5rl1EG2d17zGev1kqQe6UVItWbRTe/c0NwAuynllNpdy6pvbNYsWzMKe11/F6V7xWbdy19VL0nqkV6E1LxmuXeXbdsDawML2u5xatXPaW68fUXTrug9wIOZ2Qq1GyiTF3Zu3bDbVj8F2JlyKu5GgMz8A6U104yIWL+jfiXgQ5TrT9c29YuABcBbImL7Lq+h9bqu7vrqJUkjphchdT1wJzA7Ij7ZWhkRa1PaHwGc1lqfmfdTgmpTSq++Vv2qlP55EzvqFwE/ANYEzomIlZv6lSntjdYAzu240ffbzX7O7wi2kyjtlH6cmfe2rW+N85yIeHPbmA6nhOBtmXndEN8PSdIwDbsL+mAyc3FEfILSMfzciDiUcp1nJ0qwnJeZczse9hngl8DxEbEX5Ybf7SnXf66hhEy744H3AwdRulfcBswANqR0qPhqR/3plNZLs4E/RsRNQFC6UTwIHNnxGv636d+3H3BPRFxH6VqxNaUB7kHL965IkoajJ9OoM/MWYBvKn87YGPggJQw+xZL+fe3191EC4PuU04F7UHrwHQfs0/Tla69/ihJiZwKTgDmU60jfAj7QtEdqr3+R0n7pJMr9T3tS7nM6F9iu25/qAPan9Px7BNidElKXAltn5p3L835IkoZnwsDAsm4H0lBFxDuA++fPn8+0adP6PRxJ49ScY68Y9eece9pew3rcwoULmTVrFsAGnf1VwRtSJUkVM6QkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCStsF7858vj6nlXRCv3ewCS1CurTJrInGOvGPXnnXvaXqP+nCsqj6QkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkKuLfvpGkf+Xfk6qIf/tGkv6VR1KSpGoZUpKkahlSkqRqGVKSpGoZUtI44exRjUXO7pPGCWePaizySEqSVC1DSpJULUNKklQtQ0qSVC1DSpJULUNKfeW0aElL4xR09ZXToiUtjUdSkqRqGVKSpGoZUpKkahlSkqRqGVKSpGoZUpKkahlSkqRqGVKSpGp5M+8yRMTOwBeBLYBJwG+Bb2bmvL4OTJLGAY+kliIiDgF+CmwP/Bq4Gfg34JqIOLyPQ5OkccGQGkRErAt8B/g78N7M3D0zd6GE1NPAGRGxXj/HKEkrOkNqcEcCrwNOz8w7Wisz81bgm8DrAY+mJKmHDKnB7dosL++yrbVut1EaiySNS4ZUFxExAZgOLAb+0KXknmbbO5taSVIPOLuvuzUpp/qeyMwXOzdm5ksR8STwFmA1yjUqgIkAjz322LCf+J/PPjXsxw7XwoULR/05243H19wv4/G99jWPjuG+5rbPy4ndtk8YGBgY5pBWXBGxPvAQ8GBmvmOQmgeAtwPrZeYjzbqZwI2jM0pJWqHskJkLOld6JNVd68+2DiXB20+Z3grsADzatg9J0uAmAutQPj9fxZDqblGznLyUmta2Z1orMvMF4FXfBCRJS3XvYBucONHd05SgenNEvCrIm3VvBp7PzL+N9uAkabzwSKqLzByIiDuBrYFNgLs6SoIS8L8fieez9dLwNB1BLmCQc9mCiJgIHAEcDGxGObVyH3ApcGpmPt/H4VWneb8+AxxK+T1/FvgNcEZmXtXPsdUuIt5E+UxcNzNHbNazR1KDawXE3l22tdZd/VqfxNZLwxMR2wFn9XscNWs+cK+gvE+bAr8CrgPWBU4ErouIN/RtgHW6ADgDeAcwn/KFcUfgyoj4ch/HNRacQ/m3NaIMqcFdADwPfD4itmytjIj3Ap8DnqP8Txk2Wy8NT0TsC1wLTOn3WCp3GLAHcDuwaWbunJm7ARtTvgxtA/jB24iI/YADgQQ2ycw9MnM2MIPyO/pfEbFxP8dYq4j4d+Bjvdi3ITWIzHwAOBZYHbg5IuZFxDzgJsq9UYdn5l9e49PYemk5RMS0iLgQ+BHltNXjfR5S7Q5plkdn5p9bKzPzScopQICPj/agKnZAs/xCZr7ybysz7wQuoXxefrAfA6tZ82X7bMpn44jPajakliIzzwHmUE6TzAS2oszem52ZF4/AU9h6afmcTPmm+xtgW+Du/g6nek9S3qNbumy7p1mO+OmZMewjwObANV22rdYsXxq94YwZ51O+bB/ci507cWIZMvNK4MqR3u/ytl7KTO+6Lh+4BwMXZ+biiOj3eKqWmXOWsnmrZjk+W2900XSXuaNzfUTsCXyUcrtJty+U41ZEHEH5sn1UZv6pF7+ThlT/DLf10riVmd/o9xhWBM0XpJOaHy/r51hqFRGTgYsoXyQ3o3SgObD9NOB4FxEbAadSJpic3avn8XRf/6zaLJ9dSs1zzdIJAhpJpwDvo1zTO7XPY6nV24B9KQHV8u4+jaU6zczRH1CuQX2il2d6DKn+GW7rJWnYIuJE4AvAC8B+mflEn4dUq4WUG/bfBOxHuX/xrIj4fF9HVY/PUW6bOSYzH+rlE3m6r3+G1XpJGo6mS8rZlNmizwP7ZOYN/R1VvTJzEUt+R38YEQ9TZq99MSLOGM83QUfEFsB/AVdl5vm9fj5Dqn/+pfVSZv7LrCFbL2mkRMQU4IeUC9x/A/YyoJZPZv4qIu4FNgI25NVdaMaTrwGrAKtEROcs55UA2tYf3dzyMGyGVJ+MdusljU8RsSalo8mWwMPA7u335KloJpN8k3It6oDOL42NF5rlpFEbWJ1a18hnL6Vm/2b5JcqtEMNmSPXXPEpI7c2rQ2rEWi9pfIqIVSj/frak/PvaJTOdct5F86Vxb0o3jgvp+L2LiA0oXxwXUTpSjFuZudNg2yLiJWCivftWHD1vvaRx7UTKTc8PAzsZUMt0brM8MyKmtVY2rckupXypP3s8X4/qB4+k+igzH4iIYykXtG+OiJ83mz5A+X9z0Ai0XtI4FBFrAZ9tfnwCOH2wGy0z84CuG8afM4D3A7sDd0fEAsrv4TaUU1xXY6/DUWdI9VlmnhMRD1GOnGZSznsvAL6WmfP7OjiNZTuyZHbojOa/wRhSQGb+MyI+BHwa+A/Ke7iYcl34AuC8zFzcxyGOSxMGBuy2I0mqk9ekJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnVMqQkSdUypCRJ1TKkJEnV+n/hZhHs+gqmCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train[\"Label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run SparseCategoricalFocalLoss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the proper use of the *SparseCategoricalFocalLoss* loss function the *weights*, or classes distribution in the training set should be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df_train[\"Label\"].nunique()\n",
    "classes_dist = []\n",
    "n_of_class = df_train.groupby(\"Label\")\n",
    "for i in range(num_classes):\n",
    "    classes_dist.append(len(n_of_class.groups[i])/len(df_train))\n",
    "weights = tf.convert_to_tensor(classes_dist, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training the model\n",
    "The actual model has been trained on the [Google Colab](https://colab.research.google.com) free runtime. In order to feed the model with different training sets across the epochs *seed_batching* random seed should be changed. The variation of *learning_rate* parameter between 1e-5 and 4e-5 as well as *gamma* power from 1 to 4 did not affected validation accuracy significantly. The *amazon_my_trained_FL_G_k7e12ac78L03vac75VL04g2lr4* model was trained within 12 epochs, 11 epochs with 2000 steps and the last epoch with 7000 steps. The training step time for the given batch size was about 400 ms for P100 GPU and 800 ms for T4 GPU in full precision mode. The mixed precision mode give 400 ms per step on T4 GPU. The validation set accuracy along the epochs is slightly above 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "    loss=SparseCategoricalFocalLoss(\n",
    "        gamma=2, class_weight=weights, from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_data, epochs=1, steps_per_epoch=10, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"./amazon_my_trained_FL_G_e3x3vac77\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the actual training process at Google Colab:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
    "model.compile(optimizer=opt, \n",
    "              loss=SparseCategoricalFocalLoss(gamma=1.5, class_weight=weights, from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=3, steps_per_epoch=3333, validation_data=validation_data)\n",
    "\n",
    "Epoch 1/3\n",
    "3333/3333 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.8480WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
    "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
    "3333/3333 [==============================] - 1685s 505ms/step - loss: 0.0239 - accuracy: 0.8480 - val_loss: 0.0616 - val_accuracy: 0.7663\n",
    "Epoch 2/3\n",
    "3333/3333 [==============================] - 1679s 504ms/step - loss: 0.0390 - accuracy: 0.7762 - val_loss: 0.0520 - val_accuracy: 0.7655\n",
    "Epoch 3/3\n",
    "3333/3333 [==============================] - 1679s 504ms/step - loss: 0.0436 - accuracy: 0.7626 - val_loss: 0.0741 - val_accuracy: 0.7726\n",
    "<tensorflow.python.keras.callbacks.History at 0x7f839e8bc750>\n",
    "\n",
    "model.save_weights(\"./drive/My Drive/Bert/amazon_my_trained_FL_G_e3x3vac77\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity of train and validation loss shows absence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Outlook\n",
    "\n",
    "The model accuracy can be enhanced following ways:\n",
    "\n",
    "- Training on cloud, making the model possible to see all the training examples during each epoch in reasonable time.\n",
    "- Use human-assisted filtering, namely *HelpfulnessNumerator* and\t*HelpfulnessDenominator* dataset columns, constructing DNN classifier using last BERT layer (of size 768), *HelpfulnessNumerator* and\t*HelpfulnessDenominator* as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
