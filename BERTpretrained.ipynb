{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Use pretrained BERT for sentiment analysis\n",
    "\n",
    "For the predictive model I use pretrained bidirectional transformer [BERT](https://huggingface.co/transformers/model_doc/bert.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels= 5)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random number seed for shuffling the training set should be used for maintaining shuffling in the case of Python runtime not keeping it's state between epochs (e.g. [Google Colab](https://colab.research.google.com)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_batching = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "=================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating of Training and Validation sets\n",
    "The dataset is splitted into train (80% samples) and validation (20%) sets: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading clean and deduplicated dataset (**only first 10000 rows, for actual training remove the limitation**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dedup = pd.read_csv(\"./ReviewsDedupLowNT.csv.zip\", compression=\"zip\", index_col=0, nrows=10000)\n",
    "df_dedup = pd.read_csv(\"./ReviewsDedupLowNT.csv.zip\", compression=\"zip\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394998</th>\n",
       "      <td>these are sooooooooooooooooooooooooo delicious...</td>\n",
       "      <td>yummy. yummy, yummy!</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394999</th>\n",
       "      <td>this is a  for the price of  review because  p...</td>\n",
       "      <td>pure chocolate mallomars cookies</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395000</th>\n",
       "      <td>this would be a great coffee drink to grab out...</td>\n",
       "      <td>soso espresso style coffee drink</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395001</th>\n",
       "      <td>earths best infant formula soy iron,ounce is a...</td>\n",
       "      <td>earths best soy baby formula</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395002</th>\n",
       "      <td>taste. i was expecting it to taste pretty clos...</td>\n",
       "      <td>ehh okay i guess.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  \\\n",
       "394998  these are sooooooooooooooooooooooooo delicious...   \n",
       "394999  this is a  for the price of  review because  p...   \n",
       "395000  this would be a great coffee drink to grab out...   \n",
       "395001  earths best infant formula soy iron,ounce is a...   \n",
       "395002  taste. i was expecting it to taste pretty clos...   \n",
       "\n",
       "                                 Summary  Score  HelpfulnessNumerator  \\\n",
       "394998              yummy. yummy, yummy!      5                     0   \n",
       "394999  pure chocolate mallomars cookies      4                     0   \n",
       "395000  soso espresso style coffee drink      3                     1   \n",
       "395001      earths best soy baby formula      5                     0   \n",
       "395002                 ehh okay i guess.      2                     1   \n",
       "\n",
       "        HelpfulnessDenominator  \n",
       "394998                       3  \n",
       "394999                       0  \n",
       "395000                       2  \n",
       "395001                       0  \n",
       "395002                       1  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dedup.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_dedup.sample(frac=0.8,random_state=123)\n",
    "df_test = df_dedup.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_twocolumn_labeldecrease(input_df, data_col_name, label_col_name):\n",
    "    out_df = input_df[[data_col_name, label_col_name]]\n",
    "    out_df = out_df.rename(columns={data_col_name: \"DATA_COLUMN\", label_col_name: \"LABEL_COLUMN\"})\n",
    "    out_df[\"LABEL_COLUMN\"] = out_df[\"LABEL_COLUMN\"] - 1\n",
    "    return(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = convert_df_to_twocolumn_labeldecrease(df_train, \"Text\", \"Score\")\n",
    "test = convert_df_to_twocolumn_labeldecrease(df_test, \"Text\", \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20293</th>\n",
       "      <td>best price out there! i have been buying organ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30395</th>\n",
       "      <td>dogs loved them but then i found out theyre fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234339</th>\n",
       "      <td>nido is neato really a great tasting product a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49563</th>\n",
       "      <td>green mountain coffee has my wife pegged. this...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289396</th>\n",
       "      <td>these are the perfect size for training but th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              DATA_COLUMN  LABEL_COLUMN\n",
       "20293   best price out there! i have been buying organ...             4\n",
       "30395   dogs loved them but then i found out theyre fr...             0\n",
       "234339  nido is neato really a great tasting product a...             4\n",
       "49563   green mountain coffee has my wife pegged. this...             3\n",
       "289396  these are the perfect size for training but th...             2"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step a model predicting review score based on the review text will be created.\n",
    "For training of the BERT model the dataset samples should be converted to the *InputExample* objects, containing\n",
    "\n",
    "- *guid* - unique id for the example (not used)\n",
    "- *text_a* (string) - the untokenized text of the first sequence\n",
    "- *text_b* (optional, string) - the untokenized text of the second sequence (not used)\n",
    "- label (optional, int) - The label of the example\n",
    "\n",
    "and assembled to the TensorFlow datasets *train_data* and *validation_data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None,\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None,\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "    train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'DATA_COLUMN', \n",
    "                                                                           'LABEL_COLUMN')\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True, # Add [CLS] and [SEP]\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(buffer_size=1000, seed=seed_trainset).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Treating sample imbalance\n",
    "\n",
    "The classes in our dataset are highly imbalanced. Rating 5(Class 4) is over 60 percent of all examples with the other classes hovering around 10 percent. In order to mitigate the impact of an unbalanced training set the [*SparseCategoricalFocalLoss*](https://focal-loss.readthedocs.io/en/latest/generated/focal_loss.SparseCategoricalFocalLoss.html#focal_loss.SparseCategoricalFocalLoss) loss function is used. This function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the focusing parameter that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(text_labels[\"LABEL_COLUMN\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SparseCategoricalFocalLoss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the proper use of the *SparseCategoricalFocalLoss* loss function the *weights*, or classes distribution in the training set should be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = text_labels[\"LABEL_COLUMN\"].nunique()\n",
    "classes_dist = []\n",
    "n_of_class = text_labels.groupby(\"LABEL_COLUMN\")\n",
    "for i in range(num_classes):\n",
    "    classes_dist.append(len(n_of_class.groups[i])/len(text_labels))\n",
    "weights = tf.convert_to_tensor(classes_dist, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training the model\n",
    "The actual model has been trained on the [Google Colab](https://colab.research.google.com) free runtime. In order to feed the model with different training sets across the epochs *seed_trainset* random seed should be changed. The variation of *learning_rate* parameter between 1e-5 and 4e-5 as well as *gamma* power from 1 to 4 did not affected validation accuracy significantly. The *amazon_my_trained_FL_G_k7e12ac78L03vac75VL04g2lr4* model was trained within 12 epochs, 11 epochs with 2000 steps and the last epoch with 7000 steps (about 7% of original training set used for the training). The training step time for the given batch size was about 400 ms for P100 GPU and 800 ms for T4 GPU. The validation set accuracy along the epochs was around 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=SparseCategoricalFocalLoss(gamma=2, class_weight=weights, from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_data, epochs=1, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1590f9e10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_weights(\"./amazon_my_trained_FL_G_k7e12ac78L03vac75VL04g2lr4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Outlook\n",
    "\n",
    "The model accuracy can be enhanced following ways:\n",
    "\n",
    "- Training on cloud, making the model possible to see all the training examples at least once during the training\n",
    "- Use human-assisted filtering, namely *HelpfulnessNumerator* and\t*HelpfulnessDenominator* dataset columns, constructing DNN classifier using last BERT layer (of size 768), *HelpfulnessNumerator* and\t*HelpfulnessDenominator* as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
